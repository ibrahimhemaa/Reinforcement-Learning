{"cells":[{"cell_type":"code","execution_count":24,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-14T09:20:55.138136Z","iopub.status.busy":"2024-05-14T09:20:55.137765Z","iopub.status.idle":"2024-05-14T09:21:07.691350Z","shell.execute_reply":"2024-05-14T09:21:07.690156Z","shell.execute_reply.started":"2024-05-14T09:20:55.138106Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pygame in /opt/conda/lib/python3.10/site-packages (2.5.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install pygame"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T09:21:07.694860Z","iopub.status.busy":"2024-05-14T09:21:07.693993Z","iopub.status.idle":"2024-05-14T09:21:41.927708Z","shell.execute_reply":"2024-05-14T09:21:41.926948Z","shell.execute_reply.started":"2024-05-14T09:21:07.694811Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Simulating episode 100 && average of rewards from [1,100] = 23.76\n","Simulating episode 200 && average of rewards from [101,200] = 21.26\n","Simulating episode 300 && average of rewards from [201,300] = 23.62\n","Simulating episode 400 && average of rewards from [301,400] = 20.78\n","Simulating episode 500 && average of rewards from [401,500] = 23.31\n","Simulating episode 600 && average of rewards from [501,600] = 46.69\n","Simulating episode 700 && average of rewards from [601,700] = 60.12\n","Simulating episode 800 && average of rewards from [701,800] = 53.93\n","Simulating episode 900 && average of rewards from [801,900] = 60.08\n","Simulating episode 1000 && average of rewards from [901,1000] = 66.79\n","Simulating episode 1100 && average of rewards from [1001,1100] = 79.82\n","Simulating episode 1200 && average of rewards from [1101,1200] = 82.97\n","Simulating episode 1300 && average of rewards from [1201,1300] = 79.4\n","Simulating episode 1400 && average of rewards from [1301,1400] = 91.51\n","Simulating episode 1500 && average of rewards from [1401,1500] = 67.19\n","Simulating episode 1600 && average of rewards from [1501,1600] = 79.29\n","Simulating episode 1700 && average of rewards from [1601,1700] = 121.63\n","Simulating episode 1800 && average of rewards from [1701,1800] = 69.64\n","Simulating episode 1900 && average of rewards from [1801,1900] = 117.9\n","Simulating episode 2000 && average of rewards from [1901,2000] = 108.59\n","Simulating episode 2100 && average of rewards from [2001,2100] = 122.15\n","Simulating episode 2200 && average of rewards from [2101,2200] = 115.76\n","Simulating episode 2300 && average of rewards from [2201,2300] = 95.98\n","Simulating episode 2400 && average of rewards from [2301,2400] = 119.02\n","Simulating episode 2500 && average of rewards from [2401,2500] = 125.62\n","Simulating episode 2600 && average of rewards from [2501,2600] = 123.13\n","Simulating episode 2700 && average of rewards from [2601,2700] = 132.22\n","Simulating episode 2800 && average of rewards from [2701,2800] = 142.69\n","Simulating episode 2900 && average of rewards from [2801,2900] = 111.15\n","Simulating episode 3000 && average of rewards from [2901,3000] = 119.22\n","Simulating episode 3100 && average of rewards from [3001,3100] = 129.51\n","Simulating episode 3200 && average of rewards from [3101,3200] = 188.0\n","Simulating episode 3300 && average of rewards from [3201,3300] = 201.02\n"]}],"source":["#we have four entry in list [car_pos,car_velocity,theta,angler_velocity]\n","import gym\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","class Monte_carlo:\n","    def __init__(self,env,alpha,gamma,epsilon,numberEpisodes,numberOfBins,lowerBounds,upperBounds):\n","        self.env=env\n","        self.alpha=alpha\n","        self.gamma=gamma\n","        self.epsilon=epsilon\n","        self.actionNumber=env.action_space.n\n","        self.numberEpisodes=numberEpisodes\n","        self.numberOfBins=numberOfBins # 20 ~ 22\n","        self.lowerBounds=lowerBounds\n","        self.upperBounds=upperBounds\n","        self.sumRewardsEpisode=[]\n","        self.Qmatrix=np.zeros((numberOfBins[0],numberOfBins[1],numberOfBins[2],numberOfBins[3],self.actionNumber),dtype=object)\n","        self.cartPositionBin=np.linspace(self.lowerBounds[0],self.upperBounds[0],self.numberOfBins[0])\n","        self.cartVelocityBin=np.linspace(self.lowerBounds[1],self.upperBounds[1],self.numberOfBins[1])\n","        self.poleAngleBin=np.linspace(self.lowerBounds[2],self.upperBounds[2],self.numberOfBins[2])\n","        self.poleAngleVelocityBin=np.linspace(self.lowerBounds[3],self.upperBounds[3],self.numberOfBins[3])\n","    def returnIndexState(self,state):\n","        position =      state[0]\n","        velocity =      state[1]\n","        angle    =      state[2]\n","        angularVelocity=state[3]\n","        indexPosition=np.maximum(np.digitize(state[0],self.cartPositionBin)-1,0)\n","        indexVelocity=np.maximum(np.digitize(state[1],self.cartVelocityBin)-1,0)\n","        indexAngle=np.maximum(np.digitize(state[2],self.poleAngleBin)-1,0)\n","        indexAngularVelocity=np.maximum(np.digitize(state[3],self.poleAngleVelocityBin)-1,0)\n","        return tuple([indexPosition,indexVelocity,indexAngle,indexAngularVelocity])\n","    def selectAction(self,state,index):\n","        if index<500 or np.random.random() < self.epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            curr_index = self.returnIndexState(state)\n","            return np.argmax(self.Qmatrix[curr_index])\n","    def simulateEpisodes(self):\n","        \n","        for indexEpisode in range(self.numberEpisodes):\n","            \n","            rewardsEpisode = []\n","            (stateS,_) = self.env.reset()\n","            stateS = list(stateS)\n","            trajectory = []  # Store (state, action, reward) tuples for the episode\n","            terminalState = False\n","\n","            while not terminalState:\n","                actionA = self.selectAction(stateS, indexEpisode)\n","                stateSprime, reward, terminalState, _,_ = self.env.step(actionA)\n","                rewardsEpisode.append(reward)\n","                trajectory.append((self.returnIndexState(stateS), actionA, reward))\n","                stateS = list(stateSprime)\n","\n","\n","              # Update Q-values at the end of the episode\n","            G = 0  # Total return (cumulative rewards)\n","            gammas = [self.gamma**i for i in range(len(trajectory))]\n","            for i,(state_index,action,reward) in enumerate(reversed(trajectory)):\n","                G = reward + gammas[len(gammas)-i-1] * G\n","                self.Qmatrix[state_index + (action,)] += self.alpha * (G - self.Qmatrix[state_index + (action,)])\n","\n","            self.sumRewardsEpisode.append(np.sum(rewardsEpisode))\n","            average_reward=np.mean(self.sumRewardsEpisode[-100:])\n","            if (indexEpisode + 1) % 100 == 0:\n","                print(f\"Simulating episode {indexEpisode + 1} && average of rewards from [{indexEpisode - 100 + 2},{indexEpisode + 1}] = {average_reward}\")\n","                if average_reward>=195:\n","                    break\n","\n","            if indexEpisode%10 == 0:self.epsilon=0.99*self.epsilon\n","env=gym.make('CartPole-v1')\n","(state,_)=env.reset()\n","\n","# here define the parameters for state discretization\n","upperBounds=env.observation_space.high\n","lowerBounds=env.observation_space.low\n","cartVelocityMin=-3\n","cartVelocityMax=3\n","poleAngleVelocityMin=-10\n","poleAngleVelocityMax=10\n","upperBounds[1]=cartVelocityMax\n","upperBounds[3]=poleAngleVelocityMax\n","lowerBounds[1]=cartVelocityMin\n","lowerBounds[3]=poleAngleVelocityMin\n","\n","numberOfBinsPosition=20\n","numberOfBinsVelocity=20\n","numberOfBinsAngle=20\n","numberOfBinsAngleVelocity=20\n","numberOfBins=[numberOfBinsPosition,numberOfBinsVelocity,numberOfBinsAngle,numberOfBinsAngleVelocity]\n","\n","# define the parameters\n","alpha=0.2\n","gamma=1\n","epsilon=1\n","numberEpisodes=20000\n","\n","# create an object\n","Mc=Monte_carlo(env,alpha,gamma,epsilon,numberEpisodes,numberOfBins,lowerBounds,upperBounds)\n","Mc.simulateEpisodes()\n","env.close()"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T09:21:41.928998Z","iopub.status.busy":"2024-05-14T09:21:41.928736Z","iopub.status.idle":"2024-05-14T09:23:28.461755Z","shell.execute_reply":"2024-05-14T09:23:28.460803Z","shell.execute_reply.started":"2024-05-14T09:21:41.928975Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Simulating episode 100 && average of rewards from [1,100] = 24.08\n","Simulating episode 200 && average of rewards from [101,200] = 24.99\n","Simulating episode 300 && average of rewards from [201,300] = 21.17\n","Simulating episode 400 && average of rewards from [301,400] = 21.46\n","Simulating episode 500 && average of rewards from [401,500] = 21.94\n","Simulating episode 600 && average of rewards from [501,600] = 53.18\n","Simulating episode 700 && average of rewards from [601,700] = 62.22\n","Simulating episode 800 && average of rewards from [701,800] = 75.86\n","Simulating episode 900 && average of rewards from [801,900] = 80.4\n","Simulating episode 1000 && average of rewards from [901,1000] = 87.21\n","Simulating episode 1100 && average of rewards from [1001,1100] = 90.0\n","Simulating episode 1200 && average of rewards from [1101,1200] = 101.59\n","Simulating episode 1300 && average of rewards from [1201,1300] = 104.92\n","Simulating episode 1400 && average of rewards from [1301,1400] = 107.21\n","Simulating episode 1500 && average of rewards from [1401,1500] = 113.58\n","Simulating episode 1600 && average of rewards from [1501,1600] = 124.42\n","Simulating episode 1700 && average of rewards from [1601,1700] = 106.5\n","Simulating episode 1800 && average of rewards from [1701,1800] = 108.22\n","Simulating episode 1900 && average of rewards from [1801,1900] = 130.08\n","Simulating episode 2000 && average of rewards from [1901,2000] = 124.55\n","Simulating episode 2100 && average of rewards from [2001,2100] = 110.9\n","Simulating episode 2200 && average of rewards from [2101,2200] = 133.81\n","Simulating episode 2300 && average of rewards from [2201,2300] = 124.76\n","Simulating episode 2400 && average of rewards from [2301,2400] = 115.86\n","Simulating episode 2500 && average of rewards from [2401,2500] = 142.24\n","Simulating episode 2600 && average of rewards from [2501,2600] = 150.04\n","Simulating episode 2700 && average of rewards from [2601,2700] = 114.33\n","Simulating episode 2800 && average of rewards from [2701,2800] = 134.83\n","Simulating episode 2900 && average of rewards from [2801,2900] = 124.02\n","Simulating episode 3000 && average of rewards from [2901,3000] = 139.71\n","Simulating episode 3100 && average of rewards from [3001,3100] = 132.13\n","Simulating episode 3200 && average of rewards from [3101,3200] = 146.61\n","Simulating episode 3300 && average of rewards from [3201,3300] = 154.31\n","Simulating episode 3400 && average of rewards from [3301,3400] = 147.89\n","Simulating episode 3500 && average of rewards from [3401,3500] = 127.62\n","Simulating episode 3600 && average of rewards from [3501,3600] = 139.94\n","Simulating episode 3700 && average of rewards from [3601,3700] = 188.36\n","Simulating episode 3800 && average of rewards from [3701,3800] = 170.35\n","Simulating episode 3900 && average of rewards from [3801,3900] = 139.32\n","Simulating episode 4000 && average of rewards from [3901,4000] = 126.01\n","Simulating episode 4100 && average of rewards from [4001,4100] = 151.39\n","Simulating episode 4200 && average of rewards from [4101,4200] = 135.97\n","Simulating episode 4300 && average of rewards from [4201,4300] = 141.11\n","Simulating episode 4400 && average of rewards from [4301,4400] = 130.95\n","Simulating episode 4500 && average of rewards from [4401,4500] = 168.92\n","Simulating episode 4600 && average of rewards from [4501,4600] = 158.45\n","Simulating episode 4700 && average of rewards from [4601,4700] = 152.64\n","Simulating episode 4800 && average of rewards from [4701,4800] = 149.25\n","Simulating episode 4900 && average of rewards from [4801,4900] = 175.45\n","Simulating episode 5000 && average of rewards from [4901,5000] = 146.96\n","Simulating episode 5100 && average of rewards from [5001,5100] = 284.74\n"]}],"source":["\n","import gym\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","class Q_Learning:\n","    def __init__(self,env,alpha,gamma,epsilon,numberEpisodes,numberOfBins,lowerBounds,upperBounds):\n","        self.env=env\n","        self.alpha=alpha\n","        self.gamma=gamma\n","        self.epsilon=epsilon\n","        self.actionNumber=env.action_space.n\n","        self.numberEpisodes=numberEpisodes\n","        self.numberOfBins=numberOfBins # 20 ~ 22\n","        self.lowerBounds=lowerBounds\n","        self.upperBounds=upperBounds\n","        self.sumRewardsEpisode=[]\n","        self.Qmatrix=np.zeros((numberOfBins[0],numberOfBins[1],numberOfBins[2],numberOfBins[3],self.actionNumber),dtype=object)\n","        self.cartPositionBin=np.linspace(self.lowerBounds[0],self.upperBounds[0],self.numberOfBins[0])\n","        self.cartVelocityBin=np.linspace(self.lowerBounds[1],self.upperBounds[1],self.numberOfBins[1])\n","        self.poleAngleBin=np.linspace(self.lowerBounds[2],self.upperBounds[2],self.numberOfBins[2])\n","        self.poleAngleVelocityBin=np.linspace(self.lowerBounds[3],self.upperBounds[3],self.numberOfBins[3])\n","    def returnIndexState(self,state):\n","        position =      state[0]\n","        velocity =      state[1]\n","        angle    =      state[2]\n","        angularVelocity=state[3]\n","        indexPosition=np.maximum(np.digitize(state[0],self.cartPositionBin)-1,0)\n","        indexVelocity=np.maximum(np.digitize(state[1],self.cartVelocityBin)-1,0)\n","        indexAngle=np.maximum(np.digitize(state[2],self.poleAngleBin)-1,0)\n","        indexAngularVelocity=np.maximum(np.digitize(state[3],self.poleAngleVelocityBin)-1,0)\n","        return tuple([indexPosition,indexVelocity,indexAngle,indexAngularVelocity])\n","    def selectAction(self,state,index):\n","        if index<500 or np.random.random() < self.epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            curr_index = self.returnIndexState(state)\n","            return np.argmax(self.Qmatrix[curr_index])\n","    def simulateEpisodes(self):\n","        \n","        for indexEpisode in range(self.numberEpisodes):\n","\n","            rewardsEpisode=[]\n","            (stateS,_)=self.env.reset()\n","            stateS=list(stateS)\n","            terminalState=False\n","            while not terminalState:\n","                # return a discretized index of the state\n","                stateSIndex=self.returnIndexState(stateS)\n","\n","                actionA = self.selectAction(stateS,indexEpisode)\n","\n","\n","                # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n","                # prime means that it is the next state\n","                (stateSprime, reward, terminalState,_,_) = self.env.step(actionA)\n","\n","                rewardsEpisode.append(reward)\n","\n","                stateSprime=list(stateSprime)\n","\n","                stateSprimeIndex=self.returnIndexState(stateSprime)\n","\n","                # return the max value, we do not need actionAprime...\n","                QmaxPrime=np.max(self.Qmatrix[stateSprimeIndex])\n","\n","                if not terminalState:\n","                    error=reward+self.gamma*QmaxPrime-self.Qmatrix[stateSIndex+(actionA,)]\n","                    self.Qmatrix[stateSIndex+(actionA,)]=self.Qmatrix[stateSIndex+(actionA,)]+self.alpha*error\n","                else:\n","                    error=reward-self.Qmatrix[stateSIndex+(actionA,)]\n","                    self.Qmatrix[stateSIndex+(actionA,)]=self.Qmatrix[stateSIndex+(actionA,)]+self.alpha*error\n","                stateS=stateSprime\n","\n","            self.sumRewardsEpisode.append(np.sum(rewardsEpisode))    \n","            average_reward=np.mean(self.sumRewardsEpisode[-100:])\n","            if (indexEpisode + 1) % 100 == 0:\n","                print(f\"Simulating episode {indexEpisode + 1} && average of rewards from [{indexEpisode - 100 + 2},{indexEpisode + 1}] = {average_reward}\")\n","                if average_reward>=195:\n","                    break\n","            if indexEpisode%10 == 0:self.epsilon=0.99*self.epsilon\n","env=gym.make('CartPole-v1')\n","(state,_)=env.reset()\n","\n","# here define the parameters for state discretization\n","upperBounds=env.observation_space.high\n","lowerBounds=env.observation_space.low\n","cartVelocityMin=-3\n","cartVelocityMax=3\n","poleAngleVelocityMin=-10\n","poleAngleVelocityMax=10\n","upperBounds[1]=cartVelocityMax\n","upperBounds[3]=poleAngleVelocityMax\n","lowerBounds[1]=cartVelocityMin\n","lowerBounds[3]=poleAngleVelocityMin\n","\n","numberOfBinsPosition=20\n","numberOfBinsVelocity=20\n","numberOfBinsAngle=20\n","numberOfBinsAngleVelocity=20\n","numberOfBins=[numberOfBinsPosition,numberOfBinsVelocity,numberOfBinsAngle,numberOfBinsAngleVelocity]\n","\n","# define the parameters\n","alpha=0.2\n","gamma=1\n","epsilon=1\n","numberEpisodes=20000\n","\n","# create an object\n","Q=Q_Learning(env,alpha,gamma,epsilon,numberEpisodes,numberOfBins,lowerBounds,upperBounds)\n","Q.simulateEpisodes()\n","env.close()"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T09:23:28.465057Z","iopub.status.busy":"2024-05-14T09:23:28.464664Z","iopub.status.idle":"2024-05-14T09:24:56.556394Z","shell.execute_reply":"2024-05-14T09:24:56.555501Z","shell.execute_reply.started":"2024-05-14T09:23:28.465023Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Simulating episode 100 && average of rewards from [1,100] = 19.39\n","Simulating episode 200 && average of rewards from [101,200] = 10.34\n","Simulating episode 300 && average of rewards from [201,300] = 14.19\n","Simulating episode 400 && average of rewards from [301,400] = 14.85\n","Simulating episode 500 && average of rewards from [401,500] = 19.89\n","Simulating episode 600 && average of rewards from [501,600] = 37.75\n","Simulating episode 700 && average of rewards from [601,700] = 27.59\n","Simulating episode 800 && average of rewards from [701,800] = 37.54\n","Simulating episode 900 && average of rewards from [801,900] = 50.2\n","Simulating episode 1000 && average of rewards from [901,1000] = 52.5\n","Simulating episode 1100 && average of rewards from [1001,1100] = 56.35\n","Simulating episode 1200 && average of rewards from [1101,1200] = 38.08\n","Simulating episode 1300 && average of rewards from [1201,1300] = 150.5\n","Simulating episode 1400 && average of rewards from [1301,1400] = 129.5\n","Simulating episode 1500 && average of rewards from [1401,1500] = 51.65\n","Simulating episode 1600 && average of rewards from [1501,1600] = 49.72\n","Simulating episode 1700 && average of rewards from [1601,1700] = 49.73\n","Simulating episode 1800 && average of rewards from [1701,1800] = 64.5\n","Simulating episode 1900 && average of rewards from [1801,1900] = 112.5\n","Simulating episode 2000 && average of rewards from [1901,2000] = 56.5\n","Simulating episode 2100 && average of rewards from [2001,2100] = 104.5\n","Simulating episode 2200 && average of rewards from [2101,2200] = 38.72\n","Simulating episode 2300 && average of rewards from [2201,2300] = 39.3\n","Simulating episode 2400 && average of rewards from [2301,2400] = 83.5\n","Simulating episode 2500 && average of rewards from [2401,2500] = 123.5\n","Simulating episode 2600 && average of rewards from [2501,2600] = 102.5\n","Simulating episode 2700 && average of rewards from [2601,2700] = 57.5\n","Simulating episode 2800 && average of rewards from [2701,2800] = 110.5\n","Simulating episode 2900 && average of rewards from [2801,2900] = 56.5\n","Simulating episode 3000 && average of rewards from [2901,3000] = 89.5\n","Simulating episode 3100 && average of rewards from [3001,3100] = 51.46\n","Simulating episode 3200 && average of rewards from [3101,3200] = 58.5\n","Simulating episode 3300 && average of rewards from [3201,3300] = 79.5\n","Simulating episode 3400 && average of rewards from [3301,3400] = 55.12\n","Simulating episode 3500 && average of rewards from [3401,3500] = 110.5\n","Simulating episode 3600 && average of rewards from [3501,3600] = 55.5\n","Simulating episode 3700 && average of rewards from [3601,3700] = 82.5\n","Simulating episode 3800 && average of rewards from [3701,3800] = 53.5\n","Simulating episode 3900 && average of rewards from [3801,3900] = 65.5\n","Simulating episode 4000 && average of rewards from [3901,4000] = 59.5\n","Simulating episode 4100 && average of rewards from [4001,4100] = 106.5\n","Simulating episode 4200 && average of rewards from [4101,4200] = 123.5\n","Simulating episode 4300 && average of rewards from [4201,4300] = 929.5\n"]}],"source":["import gym\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","class Sarsa:\n","    def __init__(self,env,alpha,gamma,epsilon,numberEpisodes,numberOfBins,lowerBounds,upperBounds):\n","        self.env=env\n","        self.alpha=alpha\n","        self.gamma=gamma\n","        self.epsilon=epsilon\n","        self.actionNumber=env.action_space.n\n","        self.numberEpisodes=numberEpisodes\n","        self.numberOfBins=numberOfBins # 20 ~ 22\n","        self.lowerBounds=lowerBounds\n","        self.upperBounds=upperBounds\n","        self.sumRewardsEpisode=[]\n","        self.Qmatrix=np.zeros((numberOfBins[0],numberOfBins[1],numberOfBins[2],numberOfBins[3],self.actionNumber),dtype=object)\n","        self.cartPositionBin=np.linspace(self.lowerBounds[0],self.upperBounds[0],self.numberOfBins[0])\n","        self.cartVelocityBin=np.linspace(self.lowerBounds[1],self.upperBounds[1],self.numberOfBins[1])\n","        self.poleAngleBin=np.linspace(self.lowerBounds[2],self.upperBounds[2],self.numberOfBins[2])\n","        self.poleAngleVelocityBin=np.linspace(self.lowerBounds[3],self.upperBounds[3],self.numberOfBins[3])\n","    def returnIndexState(self,state):\n","        position =      state[0]\n","        velocity =      state[1]\n","        angle    =      state[2]\n","        angularVelocity=state[3]\n","        indexPosition=np.maximum(np.digitize(state[0],self.cartPositionBin)-1,0)\n","        indexVelocity=np.maximum(np.digitize(state[1],self.cartVelocityBin)-1,0)\n","        indexAngle=np.maximum(np.digitize(state[2],self.poleAngleBin)-1,0)\n","        indexAngularVelocity=np.maximum(np.digitize(state[3],self.poleAngleVelocityBin)-1,0)\n","        return tuple([indexPosition,indexVelocity,indexAngle,indexAngularVelocity])\n","    def selectAction(self,state,index):\n","        if index<500 or np.random.random() < self.epsilon:\n","            return self.env.action_space.sample()\n","        else:\n","            curr_index = self.returnIndexState(state)\n","            return np.argmax(self.Qmatrix[curr_index])\n","    def simulateEpisodes(self):\n","        \n","        for indexEpisode in range(self.numberEpisodes):\n","\n","            rewardsEpisode = []\n","            stateS,_ = self.env.reset()\n","            stateS = list(stateS)\n","            actionA = self.selectAction(stateS, indexEpisode)\n","            terminalState = False\n","            while not terminalState:\n","                \n","                stateSIndex = self.returnIndexState(stateS)\n","                stateSprime, reward, terminalState, _ ,_= self.env.step(actionA)\n","                rewardsEpisode.append(reward)\n","                stateSprime = list(stateSprime)\n","                stateSprimeIndex = self.returnIndexState(stateSprime)\n","                actionAprime = self.selectAction(stateSprime, indexEpisode)\n","                Qprime = self.Qmatrix[stateSprimeIndex + (actionAprime,)]\n","\n","                error = reward + self.gamma * Qprime - self.Qmatrix[stateSIndex + (actionA,)]\n","                self.Qmatrix[stateSIndex + (actionA,)] += self.alpha * error\n","\n","                stateS = stateSprime\n","                actionA = actionAprime\n","                self.sumRewardsEpisode.append(np.sum(rewardsEpisode))    \n","            average_reward=np.mean(self.sumRewardsEpisode[-100:])\n","            if (indexEpisode + 1) % 100 == 0:\n","                print(f\"Simulating episode {indexEpisode + 1} && average of rewards from [{indexEpisode - 100 + 2},{indexEpisode + 1}] = {average_reward}\")\n","                if average_reward>=195:\n","                    break\n","            if indexEpisode%10 == 0:self.epsilon=0.99*self.epsilon\n","env=gym.make('CartPole-v1')\n","(state,_)=env.reset()\n","\n","# here define the parameters for state discretization\n","upperBounds=env.observation_space.high\n","lowerBounds=env.observation_space.low\n","cartVelocityMin=-3\n","cartVelocityMax=3\n","poleAngleVelocityMin=-10\n","poleAngleVelocityMax=10\n","upperBounds[1]=cartVelocityMax\n","upperBounds[3]=poleAngleVelocityMax\n","lowerBounds[1]=cartVelocityMin\n","lowerBounds[3]=poleAngleVelocityMin\n","\n","numberOfBinsPosition=20\n","numberOfBinsVelocity=20\n","numberOfBinsAngle=20\n","numberOfBinsAngleVelocity=20\n","numberOfBins=[numberOfBinsPosition,numberOfBinsVelocity,numberOfBinsAngle,numberOfBinsAngleVelocity]\n","\n","# define the parameters\n","alpha=0.2\n","gamma=1\n","epsilon=1\n","numberEpisodes=20000\n","\n","# create an object\n","sa=Sarsa(env,alpha,gamma,epsilon,numberEpisodes,numberOfBins,lowerBounds,upperBounds)\n","sa.simulateEpisodes()\n","env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
